# Basic configuration file for a Archi deployment
# with a chat app interface, agent, and
# PostgreSQL with pgvector for document storage.
# The LLM is used through an existing Ollama server.
#
# run with:
# archi create --name my-archi-agent --config examples/deployments/basic-agent/config.yaml --services chatbot --hostmode 

name: my_archi

services:
  data_manager:
    port: 7872
  chat_app:
    agent_class: CMSCompOpsAgent
    agents_dir: examples/agents
    default_provider: local
    default_model: qwen3:32b
    providers:
      local:
        enabled: true
        base_url: http://submit76.mit.edu:7870 # make sure this matches your ollama server URL!
        mode: ollama
        default_model: "qwen3:32b" # make sure this matches a model you have downloaded locally with ollama
        models:
          - "qwen3:32b"
    trained_on: "My data"
    port: 7868
    external_port: 7868
    # Optional: Oracle database connections for agent SQL querying.
    # Uncomment and configure to enable the query_oracle_db and
    # describe_oracle_schema tools. Requires INSTALL_ORACLE=true at
    # Docker build time (or `pip install oracledb>=2.0.0`).
    #
    # tools:
    #   oracle_databases:
    #     tier0_prod:
    #       dsn: "myhost.cern.ch:1521/myservice"
    #       user: "readonly_user"
    #       password_secret: "ORACLE_TIER0_PASSWORD"   # env var or Docker secret
    #       description: "Tier-0 production database"
    #       allowed_schemas:
    #         - "CMS_WMBS"
    #         - "CMS_T0AST"
    #       max_rows: 200
    #       query_timeout_seconds: 30
    #       pool_min: 1
    #       pool_max: 5
    #     inventory:
    #       dsn: "another-host.cern.ch:1521/invservice"
    #       user: "inv_reader"
    #       password_secret: "ORACLE_INV_PASSWORD"
    #       description: "Hardware inventory database"
    #       allowed_schemas:
    #         - "HW_INV"
  vectorstore:
    backend: postgres # PostgreSQL with pgvector (only supported backend)

data_manager:
  sources:
    links:
      input_lists:
        - examples/deployments/basic-agent/miscellanea.list
  embedding_name: HuggingFaceEmbeddings

archi:
  pipelines:
    - CMSCompOpsAgent
  providers:
    local:
      enabled: true
      base_url: http://submit76.mit.edu:7870 # make sure this matches your ollama server URL!
      mode: ollama
      default_model: "qwen3:32b" # make sure this matches a model you have downloaded locally with ollama
      models:
        - "qwen3:32b"

  pipeline_map:
    CMSCompOpsAgent:
      prompts:
        required:
          agent_prompt: examples/deployments/basic-agent/agent.prompt
      models:
        required:
          agent_model: local/qwen3:32b
