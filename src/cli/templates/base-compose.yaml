services:
  {% if data_manager_enabled -%}
  data-manager:
    image: {{ data_manager_image }}:{{ data_manager_tag }}
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-data-manager
      args:
        APP_VERSION: {{ app_version }}
    container_name: {{ data_manager_container_name }}
    {% if postgres_enabled -%}
    depends_on:
      postgres:
        condition: service_healthy
      config-seed:
        condition: service_completed_successfully
    {% endif -%}
    environment:
      PGHOST: {{ 'localhost' if host_mode else 'postgres' }}
      PGPORT: {{ postgres_port }}
      PGDATABASE: archi-db
      PGUSER: archi
      PG_PASSWORD: ${PG_PASSWORD}
      VERBOSITY: {{ verbosity }}
      {% for secret in required_secrets | default([]) -%}
      {{ secret.upper() }}_FILE: /run/secrets/{{ secret.lower() }}
      {% endfor %}
    env_file:
      - .env
    secrets:
      {% for secret in required_secrets | default([]) -%}
      - {{ secret.lower() }}
      {% endfor %}
    volumes:
      - {{ data_manager_volume_name }}:/root/data/
      - ./configs:/root/archi/configs
      - ./weblists:/root/archi/weblists
    logging:
      options:
        max-size: 10m
    ports:
      - {{ data_manager_port_host }}:{{ data_manager_port_container }}
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
  {%- endif %}

  {% if postgres_enabled -%}
  postgres:
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-postgres
    container_name: {{ postgres_container_name }}
    command: ["postgres", "-p", "{{ postgres_port }}"]
    environment:
      POSTGRES_PASSWORD: ${PG_PASSWORD}
      POSTGRES_USER: archi
      POSTGRES_DB: archi-db
      PGPORT: {{ postgres_port }}
      VERBOSITY: {{ verbosity }}
    env_file:
      - .env
    volumes:
      - ./init.sql:/docker-entrypoint-initdb.d/init.sql
      - {{ postgres_volume_name }}:/var/lib/postgresql/data
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U archi -d archi-db -p {{ postgres_port }}"]
      interval: 10s
      timeout: 5s
      retries: 5
    shm_size: 1gb
  
  config-seed:
    image: {{ chatbot_image | default(benchmarking_image) }}:{{ chatbot_tag | default(benchmarking_tag) }}
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-chat
      args:
        APP_VERSION: {{ app_version }}
    container_name: {{ name }}-config-seed
    depends_on:
      postgres:
        condition: service_healthy
    environment:
      PGHOST: {{ 'localhost' if host_mode else 'postgres' }}
      PGPORT: {{ postgres_port }}
      PGDATABASE: archi-db
      PGUSER: archi
      PG_PASSWORD: ${PG_PASSWORD}
      VERBOSITY: {{ verbosity }}
      CONFIG_PATH: /rendered-config/config.yaml
    env_file:
      - .env
    volumes:
      - ./configs:/rendered-config:ro
    command: ["bash", "-lc", "python -m src.cli.tools.config_seed"]
    restart: "no"
    {%- if host_mode %}
    network_mode: host
    {%- endif %}
  {%- endif %}

  # Application services (conditional)
  {% if chatbot_enabled -%}
  chatbot:
    image: {{ chatbot_image }}:{{ chatbot_tag }}
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-chat{{ '-gpu' if gpu_ids else '' }}
      args:
        APP_VERSION: {{ app_version }}
    container_name: {{ chatbot_container_name }}
    {% if postgres_enabled or vllm_server_enabled -%}
    depends_on:
      {% if postgres_enabled -%}
      postgres:
        condition: service_healthy
      config-seed:
        condition: service_completed_successfully
      {% endif -%}
      {% if vllm_server_enabled -%}
      vllm-server:
        condition: service_healthy
      {% endif %}
    {% endif -%}
    environment:
      PGHOST: {{ 'localhost' if host_mode else 'postgres' }}
      PGPORT: {{ postgres_port }}
      PGDATABASE: archi-db
      PGUSER: archi
      PG_PASSWORD: ${PG_PASSWORD}
      VERBOSITY: {{ verbosity }}
      # Allow overriding Ollama host via env so containers can reach host daemon
      OLLAMA_HOST: ${OLLAMA_HOST:-}
      {% if vllm_server_enabled -%}
      VLLM_BASE_URL: ${VLLM_BASE_URL:-http://{{ 'localhost' if host_mode else 'vllm-server' }}:8000/v1}
      {% endif %}
      {% for secret in required_secrets | default([]) -%}
      {{ secret.upper() }}_FILE: /run/secrets/{{ secret.lower() }}
      {% endfor %}
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {% endif %}
    env_file:
      - .env
    {% if gpu_ids and not use_podman -%}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    {% endif %}
    secrets:
      {% for secret in required_secrets | default([]) -%}
      - {{ secret.lower() }}
      {% endfor %}
    volumes:
      - {{ data_manager_volume_name }}:/root/data/
      - ./configs:/root/archi/configs
      - ./data/prompts:/root/archi/data/prompts:ro
      - ./data/agents:/root/archi/agents
      {% for prompt_file in prompt_files | default([]) -%}
      - ./{{ prompt_file }}:/root/archi/{{ prompt_file }}
      {% endfor %}
      {% if gpu_ids -%}
      - archi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    ports:
      - {{ chatbot_port_host }}:{{ chatbot_port_container }}
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    security_opt:
      - label:disable
    devices:
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}
  {%- endif %}

  {% if grafana_enabled -%}
  grafana:
    image: {{ grafana_image }}:{{ grafana_tag }}
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-grafana
    container_name: {{ grafana_container_name }}
    {% if postgres_enabled -%}
    depends_on:
      postgres:
        condition: service_healthy
      config-seed:
        condition: service_completed_successfully
    {% endif -%}
    environment:
      PGHOST: {{ 'localhost' if host_mode else 'postgres' }}
      PGPORT: {{ postgres_port }}
      PGDATABASE: archi-db
      PGUSER: archi
      PG_PASSWORD: ${PG_PASSWORD}
      VERBOSITY: {{ verbosity }}
    ports:
      - {{ grafana_port_host }}:3000
    volumes:
      - {{ grafana_volume_name }}:/var/lib/grafana
      - ./grafana/archi-default-dashboard.json:/var/lib/grafana/dashboards/archi-default-dashboard.json
      - ./grafana/datasources.yaml:/etc/grafana/provisioning/datasources/datasources.yaml
      - ./grafana/dashboards.yaml:/etc/grafana/provisioning/dashboards/dashboards.yaml
      - ./grafana/grafana.ini:/etc/grafana/grafana.ini
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
  {%- endif %}

  {% if grader_enabled -%}
  grader:
    image: {{ grader_image }}:{{ grader_tag }}
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-grader{{ '-gpu' if gpu_ids else '' }}
    {% if postgres_enabled -%}
    depends_on:
      postgres:
        condition: service_healthy
      config-seed:
        condition: service_completed_successfully
    {% endif -%}
    environment:
      PGHOST: {{ 'localhost' if host_mode else 'postgres' }}
      PGPORT: {{ postgres_port }}
      PGDATABASE: archi-db
      PGUSER: archi
      PG_PASSWORD: ${PG_PASSWORD}
      VERBOSITY: {{ verbosity }}
    environment:
      {% for secret in required_secrets | default([]) -%}
      {{ secret.upper() }}_FILE: /run/secrets/{{ secret.lower() }}
      {% endfor %}
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {% endif %}
      VERBOSITY: {{ verbosity }}
      USERS_FILE: /root/archi/users.csv
      {% for rubric in rubrics | default([]) -%}
      {{ rubric.upper() }}_FILE: /root/archi/{{ rubric }}.txt
      {% endfor %}
    env_file:
      - .env
    {% if gpu_ids and not use_podman -%}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    {% endif %}
    secrets:
      {% for secret in required_secrets | default([]) -%}
      - {{ secret.lower() }}
      {% endfor %}
    ports: 
      - {{ grader_port_host }}:{{ grader_port_container }}
    volumes:
      - {{ grader_volume_name }}:/root/data/
      - ./configs:/root/archi/configs
      - ./data/prompts:/root/archi/data/prompts:ro
      - ./data/agents:/root/archi/agents:ro
      {% for prompt_file in prompt_files | default([]) -%}
      - ./{{ prompt_file }}:/root/archi/{{ prompt_file }}
      {% endfor %}
      - ./users.csv:/root/archi/users.csv
      {%- for rubric in rubrics | default([]) %}
      - ./{{ rubric }}.txt:/root/archi/{{ rubric }}.txt
      {%- endfor %}
      {% if gpu_ids -%}
      - archi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    security_opt:
      - label:disable
    devices:
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}
  {%- endif %}

  {% if piazza_enabled -%}
  piazza:
    image: {{ piazza_image }}:{{ piazza_tag }}
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-piazza
    container_name: {{ piazza_container_name }}
    {% if postgres_enabled -%}
    depends_on:
      postgres:
        condition: service_healthy
      config-seed:
        condition: service_completed_successfully
    {% endif -%}
    environment:
      {% for secret in required_secrets | default([]) -%}
      {{ secret.upper() }}_FILE: /run/secrets/{{ secret.lower() }}
      {% endfor %}
      VERBOSITY: {{ verbosity }}
    env_file:
      - .env
    secrets:
      {% for secret in required_secrets | default([]) -%}
      - {{ secret.lower() }}
      {% endfor %}
    volumes:
      - {{ data_volume_name }}:/root/data/
      - ./configs:/root/archi/configs
      - ./data/prompts:/root/archi/data/prompts:ro
      - ./data/agents:/root/archi/agents:ro
      {% for prompt_file in prompt_files | default([]) -%}
      - ./{{ prompt_file }}:/root/archi/{{ prompt_file }}
      {% endfor %}
      {% if gpu_ids -%}
      - archi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
  {%- endif %}

  {% if mattermost_enabled -%}
  mattermost:
    image: {{ mattermost_image }}:{{ mattermost_tag }}
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-mattermost{{ '-gpu' if gpu_ids else '' }}
      args:
        TAG: {{ mattermost_tag }}
    {% if postgres_enabled -%}
    depends_on:
      postgres:
        condition: service_healthy
      config-seed:
        condition: service_completed_successfully
    {% endif -%}
    environment:
      {% for secret in required_secrets | default([]) -%}
      {{ secret.upper() }}_FILE: /run/secrets/{{ secret.lower() }}
      {% endfor %}
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {% endif %}
      VERBOSITY: {{ verbosity }}
    env_file:
      - .env
    {% if gpu_ids and not use_podman -%}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    {% endif %}
    secrets:
      {% for secret in required_secrets | default([]) -%}
      - {{ secret.lower() }}
      {% endfor %}
    volumes:
      - {{ data_volume_name }}:/root/data/
      - ./configs:/root/archi/configs
      - ./data/prompts:/root/archi/data/prompts:ro
      - ./data/agents:/root/archi/agents:ro
      {% for prompt_file in prompt_files | default([]) -%}
      - ./{{ prompt_file }}:/root/archi/{{ prompt_file }}
      {% endfor %}
      {% if gpu_ids -%}
      - archi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    security_opt:
      - label:disable
    devices:
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}
  {%- endif %}

  {% if redmine_mailer_enabled -%}
  redmine:
    image: {{ redmine_mailer_image }}-redmine:{{ redmine_mailer_tag }}
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-redmine{{ '-gpu' if gpu_ids else '' }}
    container_name: {{ redmine_mailer_container_name }}-redmine
    {% if postgres_enabled -%}
    depends_on:
      postgres:
        condition: service_healthy
      config-seed:
        condition: service_completed_successfully
    {% endif -%}
    environment:
      {% for secret in required_secrets | default([]) -%}
      {{ secret.upper() }}_FILE: /run/secrets/{{ secret.lower() }}
      {% endfor %}
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {% endif %}
      VERBOSITY: {{ verbosity }}
    env_file:
      - .env
    secrets:
      {% for secret in required_secrets | default([]) -%}
      - {{ secret.lower() }}
      {% endfor %}
    volumes:
      - {{ data_volume_name }}:/root/data/
      - ./configs:/root/archi/configs
      - ./data/prompts:/root/archi/data/prompts:ro
      - ./data/agents:/root/archi/agents:ro
      {% for prompt_file in prompt_files | default([]) -%}
      - ./{{ prompt_file }}:/root/archi/{{ prompt_file }}
      {% endfor %}
      {% if gpu_ids -%}
      - archi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    security_opt:
      - label:disable
    devices:
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}

  mailbox:
    image: {{ redmine_mailer_image }}-mailer:{{ redmine_mailer_tag }}
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-mailbox{{ '-gpu' if gpu_ids else '' }}
    container_name: {{ redmine_mailer_container_name }}-mailer
    {% if postgres_enabled -%}
    depends_on:
      postgres:
        condition: service_healthy
      config-seed:
        condition: service_completed_successfully
    {% endif -%}
    environment:
      {% for secret in required_secrets | default([]) -%}
      {{ secret.upper() }}_FILE: /run/secrets/{{ secret.lower() }}
      {% endfor %}
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {% endif %}
      VERBOSITY: {{ verbosity }}
    env_file:
      - .env
    secrets:
      {% for secret in required_secrets | default([]) -%}
      - {{ secret.lower() }}
      {% endfor %}
    volumes:
      - ./config.yaml:/root/archi/config.yaml
      - ./data/prompts:/root/archi/data/prompts:ro
      - ./data/agents:/root/archi/agents:ro
      {% for prompt_file in prompt_files | default([]) -%}
      - ./{{ prompt_file }}:/root/archi/{{ prompt_file }}
      {% endfor %}
      {% if gpu_ids -%}
      - archi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    restart: always
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    security_opt:
      - label:disable
    devices:
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}
  {%- endif %}

  {% if vllm_server_enabled -%}
  vllm-server:
    image: vllm/vllm-openai:latest
    container_name: vllm-server-{{ name }}
    {% if not host_mode -%}
    ports:
      - "8000:8000"
    {% endif -%}
    environment:
      NCCL_P2P_DISABLE: "1"
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        SHM_SIZE=$$(df /dev/shm | awk 'NR==2 {print $$2}')
        if [ "$$SHM_SIZE" -lt 1048576 ] 2>/dev/null; then
          echo "WARNING: /dev/shm is $$(( $$SHM_SIZE / 1024 ))MB â€” less than 1GB. Use ipc: host or increase shm_size for stable multi-GPU inference."
        fi
        exec python3 -m vllm.entrypoints.openai.api_server \
          --model "{{ vllm_model | default('Qwen/Qwen2.5-7B-Instruct-1M') }}" \
          --enable-auto-tool-choice \
          --tool-call-parser "{{ vllm_tool_parser | default('hermes') }}" \
          {% if vllm_gpu_memory_utilization is defined %}--gpu-memory-utilization {{ vllm_gpu_memory_utilization }} {% endif %}\
          {% if vllm_max_model_len is defined %}--max-model-len {{ vllm_max_model_len }} {% endif %}\
          {% if vllm_tensor_parallel_size is defined %}--tensor-parallel-size {{ vllm_tensor_parallel_size }} {% endif %}\
          {% if vllm_dtype is defined %}--dtype {{ vllm_dtype }} {% endif %}\
          {% if vllm_quantization is defined %}--quantization {{ vllm_quantization }} {% endif %}\
          {% if vllm_enforce_eager is defined and vllm_enforce_eager %}--enforce-eager {% endif %}\
          {% if vllm_max_num_seqs is defined %}--max-num-seqs {{ vllm_max_num_seqs }} {% endif %}\
          {% if vllm_enable_prefix_caching is defined and not vllm_enable_prefix_caching %}--no-enable-prefix-caching {% endif %}\
          {% for key, val in vllm_engine_args.items() %}{% if val != '' %}--{{ key }} {{ val }} {% else %}--{{ key }} {% endif %}{% endfor %}

    ipc: host
    {% if host_mode -%}
    network_mode: host
    {% endif -%}
    ulimits:
      memlock:
        soft: -1
        hard: -1
      stack: 67108864
    {% if not use_podman -%}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    {% endif %}
    {% if use_podman -%}
    security_opt:
      - label:disable
    devices:
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:8000/v1/models || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 20
      start_period: 120s
    logging:
      options:
        max-size: 10m
    restart: always
  {%- endif %}

  {% if benchmarking_enabled -%}
  benchmark:
    image: {{ benchmarking_image }}:{{ benchmarking_tag }}
    build:
      context: .
      dockerfile: archi_code/cli/templates/dockerfiles/Dockerfile-benchmarks{{ '-gpu' if gpu_ids else '' }}
    container_name: {{ benchmarking_container_name }}
    {% if postgres_enabled -%}
    depends_on:
      postgres:
        condition: service_healthy
      config-seed:
        condition: service_completed_successfully
    {% endif -%}
    environment:
      PGHOST: {{ 'localhost' if host_mode else 'postgres' }}
      PGPORT: {{ postgres_port }}
      PGDATABASE: archi-db
      PGUSER: archi
      PG_PASSWORD: ${PG_PASSWORD}
      {% for secret in required_secrets | default([]) -%}
      {{ secret.upper() }}_FILE: /run/secrets/{{ secret.lower() }}
      {% endfor %}
      {% if gpu_ids -%}
      NVIDIA_VISIBLE_DEVICES: all
      NVIDIA_DRIVER_CAPABILITIES: compute,utility,graphics
      {% endif %}
      VERBOSITY: {{ verbosity }}
      container_name: {{ benchmarking_container_name }}
    env_file:
      - .env
    {% if gpu_ids and not use_podman -%}
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    {% endif %}
    secrets:
      {% for secret in required_secrets | default([]) -%}
      - {{ secret.lower() }}
      {% endfor %}
    volumes:
      - {{ benchmarking_volume_name }}:/root/data/
      - ./queries.txt:/root/archi/QandA.txt
      - {{ benchmarking_dest }}:/root/archi/benchmarks
      - ./data/prompts:/root/archi/data/prompts:ro
      - ./data/agents:/root/archi/agents:ro
      {% for prompt_file in prompt_files | default([]) -%}
      - ./{{ prompt_file }}:/root/archi/{{ prompt_file }}
      {% endfor %}
      {% if gpu_ids -%}
      - archi-models:/root/models/
      {%- endif %}
    logging:
      options:
        max-size: 10m
    {% if host_mode -%}
    network_mode: host
    {% endif %}
    {% if gpu_ids and use_podman -%}
    security_opt:
      - label:disable
    devices:
    {%- if gpu_ids == "all" %}
      - "nvidia.com/gpu=all"
    {%- else %}
      {%- for gpu_id in gpu_ids %}
      - "nvidia.com/gpu={{ gpu_id }}"
      {%- endfor %}
    {%- endif %}
    {%- endif %}
  {%- endif %}


volumes:
  {% for volume in required_volumes -%}
  {{ volume }}:
    external: true
  {% endfor %}

{% if required_secrets %}
secrets:
  {% for secret in required_secrets -%}
  {{ secret.lower() }}:
    file: secrets/{{ secret.lower() }}.txt
  {% endfor %}
{% endif %}
